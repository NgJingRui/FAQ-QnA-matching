{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rjkin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rjkin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rjkin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patterns import pattern_specs\n",
    "from patterns import TOKENS as SPECIAL_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFunc(pattern_list : list):\n",
    "    \"\"\"\n",
    "    pattern_list is a list of dicts ,\n",
    "    each dict has keys\n",
    "    1) patterns  ---> a list of strings \n",
    "    2) substitution_keys --> strings to substute the {word}\n",
    "    3) description --> lite\n",
    "    \"\"\"\n",
    "    def parser(S : list , index : int , answer : list, sub_dict : dict):\n",
    "        \"\"\"\n",
    "        parse a list of string , if we find a token that is substutiable we \n",
    "        substitute and move on , finally add all the possible substuted strings to answer (list of list)\n",
    "        \"\"\"\n",
    "        if(index == len(S) -1):\n",
    "            answer.append(S)\n",
    "            return\n",
    "        if(S[index][0] == \"{\"):\n",
    "            # this is a substition token ex {aux1}\n",
    "            sub_token = S[index][1:-1]\n",
    "            assert sub_token in sub_dict , \"sub token not in sub dict\"\n",
    "            \n",
    "            # now itreating over all the possible replacements\n",
    "            for sub in sub_dict[sub_token]:\n",
    "                S1 = S.copy()\n",
    "                S1[index] = sub\n",
    "                parser(S1,index+ 1,answer,sub_dict)\n",
    "        \n",
    "        else:\n",
    "            parser(S,index+1,answer,sub_dict)\n",
    "            \n",
    "    \n",
    "    generated = []\n",
    "    for pattern_dict in pattern_list:\n",
    "        # iterating through the dicts\n",
    "        patterns = pattern_dict[\"patterns\"]\n",
    "        sub_dict = pattern_dict[\"substitution_keys\"]\n",
    "        gens = [] # generated patters for all the patterns in the pattern_dict[\"patters\"]\n",
    "        for pattern in patterns:\n",
    "            accumulator = []  # to accumulate all the substuted patterns \n",
    "            pattern = pattern.strip().split()\n",
    "            parser(pattern,0,accumulator,sub_dict)\n",
    "            \n",
    "            # acccumulator is a list of lists\n",
    "            accumulator = [\" \".join(acc) for acc in accumulator]\n",
    "            #now accumulator is a list of strings\n",
    "            gens.extend(accumulator)\n",
    "        \n",
    "        generated.append(gens)\n",
    "    \n",
    "    return generated\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_model = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_punct(sentence, replace_with=\"\", ignore_list=\"\"):\n",
    "    \"\"\" Replace punctuation in `sentence` with tokens specified by `replace_with`.\n",
    "    \"\"\"\n",
    "    punct = string.punctuation\n",
    "    for symbol in ignore_list:\n",
    "        punct = punct.replace(symbol, '')\n",
    "    if replace_with:\n",
    "        return sentence.translate(str.maketrans(punct, replace_with * len(punct)))\n",
    "    else:\n",
    "        return sentence.translate(str.maketrans('', '', punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(sentence, n):\n",
    "    temp = sentence.split()\n",
    "    return [' '.join(temp[i:i + n]) for i in range(len(temp) - n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "import regex\n",
    "\n",
    "_spacy_docs_cache = dict()\n",
    "\n",
    "\n",
    "class MatchedResult:\n",
    "    def __init__(self, group_id, pattern, match):\n",
    "        self.group_id = group_id\n",
    "        self.pattern = pattern\n",
    "        self.fuzzy_counts = sum(match.fuzzy_counts)\n",
    "        self.tokens = self._parse_tokens(match.string, pattern, match)\n",
    "        self.mh = match\n",
    "    @staticmethod\n",
    "    def _get_spacy_doc(sentence):\n",
    "        spacy_nlp = spacy_model\n",
    "        if sentence not in _spacy_docs_cache:\n",
    "            with spacy_nlp.disable_pipes('ner'):\n",
    "                _spacy_docs_cache[sentence] = spacy_nlp(sentence)\n",
    "\n",
    "        return _spacy_docs_cache[sentence]\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_first_verb_offset(sentence, starting_offset=0):\n",
    "        \"\"\" Return character offset for the first encountered verb\n",
    "        \"\"\"\n",
    "        doc = MatchedResult._get_spacy_doc(sentence)\n",
    "        for token in doc:\n",
    "            if token.idx < starting_offset:\n",
    "                continue\n",
    "            # single verb or adverb-verb pair\n",
    "            elif token.pos_ == 'VERB' or (\n",
    "                    token.tag_ == 'RB' and (token.i + 1 < len(doc) and doc[token.i + 1].pos_ == 'VERB')\n",
    "            ):\n",
    "                return token.idx\n",
    "\n",
    "        return -1\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_subj(sentence, substr):\n",
    "        \"\"\" Returns True if `substr` in `sentence` is a noun chunk and its syntactic dependency tag is nominal subject\n",
    "        \"\"\"\n",
    "        # remove punctuations to make things easier\n",
    "        sentence = \" \".join(replace_punct(sentence, replace_with=\" \", ignore_list=\"-\").strip().split())\n",
    "        substr = \" \".join(replace_punct(substr, replace_with=\" \", ignore_list=\"-\").strip().split())\n",
    "\n",
    "        doc = MatchedResult._get_spacy_doc(sentence)\n",
    "        temp = []\n",
    "        sbj_tags = ['nsubj', 'nsubjpass']\n",
    "        previous_dep = None\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if chunk.text == substr and chunk.root.dep_ in sbj_tags:\n",
    "                return True\n",
    "            elif chunk.text in get_ngrams(substr, len(chunk.text.split())):\n",
    "                # sometimes the subject might be combination of chunks, so we're keeping this chunk for later\n",
    "                # *Note: Consecutive subject chunk is discarded since they are two separate and not\n",
    "                # related noun chunks. (i.e. If they belongs to same chunk, they'd be grouped together as\n",
    "                # a chunk, not as 2 separate chunks)\n",
    "                if not (previous_dep in sbj_tags and chunk.root.dep_ in sbj_tags):\n",
    "                    temp.append(chunk.text)\n",
    "                    previous_dep = chunk.root.dep_\n",
    "\n",
    "        if temp and substr.startswith(temp[0]) and substr.endswith(temp[-1]):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_tokens(sentence, pattern, match):\n",
    "        tokens = defaultdict(list)\n",
    "        for t in SPECIAL_TOKENS:\n",
    "            if t in pattern:\n",
    "                tokens[t] = match.captures(t[1:-1])\n",
    "        \n",
    "        # resolve <sbj> <act> and <obj> <act>\n",
    "        if \"<sbj> <act>\" in pattern or \"<obj> <act>\" in pattern:\n",
    "            pairs = list(product(tokens['<sbj>'] if \"<sbj> <act>\" in pattern else tokens['<obj>'], tokens['<act>']))\n",
    "            unresolved = []\n",
    "            for pair in pairs:\n",
    "                if \" \".join(pair) in sentence:\n",
    "                    unresolved.append(pair)\n",
    "\n",
    "            for pair in unresolved:\n",
    "                sub_sentence = \" \".join(pair)\n",
    "                verb_offset = MatchedResult._get_first_verb_offset(sentence, sentence.index(sub_sentence))\n",
    "                if verb_offset == -1:\n",
    "                    tokens = defaultdict(list)\n",
    "                    break\n",
    "\n",
    "                second_part = sentence[verb_offset:sentence.index(sub_sentence) + len(sub_sentence)]\n",
    "                first_part = sub_sentence[:sub_sentence.rfind(second_part)]\n",
    "\n",
    "                if pair[0] in tokens['<sbj>']:\n",
    "                    index = tokens['<sbj>'].index(pair[0])\n",
    "                    tokens['<sbj>'][index] = first_part\n",
    "                else:\n",
    "                    index = tokens['<obj>'].index(pair[0])\n",
    "                    tokens['<obj>'][index] = first_part\n",
    "\n",
    "                act_index = tokens['<act>'].index(pair[1])\n",
    "                tokens['<act>'][act_index] = second_part\n",
    "\n",
    "        # Verify <sbj> tokens\n",
    "        for sbj in tokens['<sbj>']:\n",
    "            if not MatchedResult._is_subj(sentence, sbj):\n",
    "                tokens = defaultdict(list)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class FuzzyMatcher:\n",
    "    def __init__(self, patterns):\n",
    "        self.patterns = [[(pattern, self._convert_to_fuzzy_regex(pattern)) for pattern in group] for group in patterns]\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_to_fuzzy_regex(pattern):\n",
    "        match_any = '.+'\n",
    "        fuzzy_rule = '{e<=3}'\n",
    "        ignore_case = '(?i)'\n",
    "        space = re.escape(' ')\n",
    "        placeholder = '<<<placeholder>>>'\n",
    "\n",
    "        special_tokens = [token for token in pattern.split() if token in SPECIAL_TOKENS]\n",
    "        pattern = re.escape(pattern)\n",
    "        for token in SPECIAL_TOKENS:\n",
    "            pattern = pattern.replace(re.escape(token), match_any)\n",
    "\n",
    "        parts = pattern.split(match_any)\n",
    "        for i in range(len(parts)):\n",
    "            part = parts[i]\n",
    "            if len(part.strip(space)) != 0:\n",
    "                add_start = part.startswith(space)\n",
    "                add_end = part.endswith(space)\n",
    "                parts[i] = f\"{space if add_start else ''}({part.strip(space)}){fuzzy_rule}{space if add_end else ''}\"\n",
    "\n",
    "        fuzzy_regex = rf\"{ignore_case}{match_any.join(parts)}\"\n",
    "        # Add named capture groups\n",
    "        for sp in special_tokens:\n",
    "            fuzzy_regex = fuzzy_regex.replace(match_any, f\"(?{sp}{placeholder})\", 1)\n",
    "        fuzzy_regex = fuzzy_regex.replace(placeholder, match_any)\n",
    "\n",
    "        return fuzzy_regex\n",
    "\n",
    "    def match(self, sentence):\n",
    "        def get_effective_length(_pattern):\n",
    "            # get pattern length without counting special tokens\n",
    "            _pattern = _pattern.replace(\"<sbj>\", \"\")\n",
    "            _pattern = _pattern.replace(\"<obj>\", \"\")\n",
    "            _pattern = _pattern.replace(\"<act>\", \"\")\n",
    "            _pattern = _pattern.replace(\"<st>\", \"\")\n",
    "\n",
    "            return len(_pattern)\n",
    "\n",
    "        candidates = []\n",
    "        for group_id, group in enumerate(self.patterns):\n",
    "            for pattern, fuzzy_regex in group:\n",
    "                match = regex.fullmatch(fuzzy_regex, sentence.strip(\"?\"))\n",
    "                if match is not None:\n",
    "                    candidates.append(MatchedResult(group_id, pattern, match))\n",
    "\n",
    "        # sort by least fuzzy count, then by most pattern length\n",
    "        candidates.sort(key=lambda x: (x.fuzzy_counts, -get_effective_length(x.pattern)))\n",
    "\n",
    "        return candidates[0] if len(candidates) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = FuzzyMatcher(myFunc(pattern_specs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = matcher.match(\"any examples for  baby bonus?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'obj': ['examples for  baby bonus']}"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mh.capturesdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
